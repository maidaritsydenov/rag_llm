{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG\n",
    "`RAG - это техника, повышающая производительность языковых моделей путём предоставления модели контекста вместе с вопросом.`\n",
    "\n",
    "Последовательность действий:\n",
    "1. Передадим модели информацию о нашем заводе без дополнительного тюнинга;\n",
    "2. Создадим базу векторов, где будут храниться ембеддинги ранее заданных вопросов (кэш);\n",
    "3. При обращении к модели, будем проверять, задавались ли ранее похожие вопросы. Если да, то отдаём ранее сгенерированные ответы.\n",
    "\n",
    "Зачем использовать кэш?\n",
    "* Чтобы увеличить скорость ответов для вопросов, которые задавались ранее.\n",
    "* Снизить затраты при использовании платных API (GTP-3.5, GPT-4) для ответов на однотипные и повторяющиеся вопросы.\n",
    "\n",
    "В качестве модели мы будем использовать адаптивные веса русскоязычной \"saiga_mistral_7b_lora\" (https://huggingface.co/IlyaGusev/saiga_mistral_7b_lora) (Лицензия CC BY 4.0), которые натренировал Илья Гусев @Takagi\n",
    "\n",
    "Оригинальная модель - \"Mistral-7B-OpenOrca\" (https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# !pip install faiss-cpu\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "cache_dir = r'E:\\dev\\0_MODELS_HF'\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = cache_dir\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# db\n",
    "answers_file_path = './database/answers.txt'\n",
    "emb_file_path = \"./database/emb_database.npy\"\n",
    "\n",
    "# models\n",
    "base_model_name = \"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "adapt_model_name = \"IlyaGusev/saiga_mistral_7b_lora\"\n",
    "\n",
    "sentence_model_name = 'sentence-transformers/all-MiniLM-L6-v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем модель для генерации ответа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505ba1b30b2c4a05baea98a31dc875d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load models and tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_name,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    adapt_model_name,\n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем модель для получения эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenizer = AutoTokenizer.from_pretrained(sentence_model_name, cache_dir=cache_dir)\n",
    "sent_model = AutoModel.from_pretrained(sentence_model_name, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для получения эмбеддингов.\n",
    "# На вход подаём строку, на выходе получаем torch.tensor размерностью (1, 384)\n",
    "\n",
    "def get_embedding(sentence):\n",
    "    \n",
    "    # Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def _mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    encoded_input = sent_tokenizer([sentence], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = sent_model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = _mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаём базу для эмбеддингов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функции для сохранения и загрузки db\n",
    "def update_db(answers, emb_database):\n",
    "    with open(answers_file_path, 'w') as file:\n",
    "        file.writelines(answer + '\\n' for answer in answers)\n",
    "    np.save(emb_file_path, emb_database.numpy())\n",
    "\n",
    "def load_db(answers_file_path, emb_file_path):\n",
    "    if os.path.exists(answers_file_path) and os.path.exists(emb_file_path):\n",
    "        with open(answers_file_path, 'r') as file:\n",
    "            answers = [line.strip() for line in file.readlines()]\n",
    "        emb_database = torch.tensor(np.load(emb_file_path), dtype=torch.float32)\n",
    "    else:\n",
    "        answers = []\n",
    "        emb_database = torch.empty((0, 384), dtype=torch.float32)\n",
    "    return answers, emb_database    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим краткое описание вымышленной компании, которое будем подавать модели вместе с вопросами, чтобы модель была в курсе контекста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "user: Компания \"Стальной Щит\" специализируется на разработке и производстве передовой военной техники. Наше предприятие расположено на улице Технологическая, 123, в городе Защитоград. До компании Вы можете добраться до нас следующими способами: 1. На метро: станция \"Вымышленная\", 10 выход, прямо и налево. 2. На автобусе: автобус №100 \"Атаковоево - Защитоград\". С конечной остановки прямо и направо. Мы занимаемся созданием инновационных боевых машин, включая танки, боевые вертолеты, беспилотные летательные аппараты и системы киберзащиты. Компания \"Стальной Щит\" стремится к высочайшему качеству продукции, применяя передовые технологии и сотрудничая с лучшими специалистами в области военной промышленности. Мы гордимся нашими инновационными решениями, обеспечивающими безопасность и надежность наших вооруженных сил. Вы можете связаться с нами по телефону +1234567890 или по электронной почте info@steelshield.com, чтобы узнать больше о нашей продукции, услугах и возможностях сотрудничества. Наши специалисты готовы ответить на ваши вопросы о нашей технике, ее характеристиках, ценах и условиях поставки. {question}\\nbot: Вот ответ на ваш вопрос длиной не более 10 слов:\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обернём промт в PromptTemplate из библиотеки langchain. Это аналог f-строки, только с возможностью передавать строку, как зависимую переменную, с последующей передачей ей аргумента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_prompt_less10 = PromptTemplate.from_template(r'user: Компания \"Стальной Щит\" специализируется на разработке и производстве передовой военной техники. Наше предприятие расположено на улице Технологическая, 123, в городе Защитоград. До компании Вы можете добраться до нас следующими способами: 1. На метро: станция \"Вымышленная\", 10 выход, прямо и налево. 2. На автобусе: автобус №100 \"Атаковоево - Защитоград\". С конечной остановки прямо и направо. Мы занимаемся созданием инновационных боевых машин, включая танки, боевые вертолеты, беспилотные летательные аппараты и системы киберзащиты. Компания \"Стальной Щит\" стремится к высочайшему качеству продукции, применяя передовые технологии и сотрудничая с лучшими специалистами в области военной промышленности. Мы гордимся нашими инновационными решениями, обеспечивающими безопасность и надежность наших вооруженных сил. Вы можете связаться с нами по телефону +1234567890 или по электронной почте info@steelshield.com, чтобы узнать больше о нашей продукции, услугах и возможностях сотрудничества. Наши специалисты готовы ответить на ваши вопросы о нашей технике, ее характеристиках, ценах и условиях поставки. {question}\\nbot: Вот ответ на ваш вопрос длиной не более 10 слов:\")')\n",
    "\n",
    "\n",
    "# Функция для генерации ответа моделью и парсинг ответа\n",
    "def get_answer(info_prompt, question):\n",
    "    \n",
    "    prompt = info_prompt.format(question=question)   \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), \n",
    "                            top_p=0.5,\n",
    "                            temperature=0.3,\n",
    "                            attention_mask=inputs[\"attention_mask\"],\n",
    "                            max_new_tokens=50,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            do_sample=True)\n",
    "\n",
    "    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    parsed_answer = output.split(\"Вот ответ на ваш вопрос длиной не более 10 слов:\")[1].strip()\n",
    "\n",
    "    if \"bot:\" in parsed_answer:\n",
    "        parsed_answer = parsed_answer.split(\"bot:\")[0].strip()\n",
    "\n",
    "    return parsed_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пайплайн:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cos_sim(emb, emb_database):\n",
    "    return F.cosine_similarity(emb_database, emb, dim=1, eps=1e-8)\n",
    "\n",
    "def pipe(question, answers):\n",
    "    global emb_database\n",
    "    emb = get_embedding(question)                                       # Создаем эмбеддинг вопроса\n",
    "    cos_sim = get_cos_sim(emb, emb_database)\n",
    "    if cos_sim.numel() == 0:\n",
    "        cos_sim = torch.tensor([0.0])\n",
    "    max_value, max_index = torch.max(cos_sim, dim=0)                    # Получаем самый похожий вопрос и индекс сохраненного ответа\n",
    "\n",
    "    if max_value > 0.83:\n",
    "        answer = answers[max_index]\n",
    "        print(f'cos_sim={max_value}\\nОтвет из DB: {answer}')            # Если есть похожий вопрос то выдаем закэшированный ответ из БД\n",
    "    else:\n",
    "        answer = get_answer(info_prompt_less10, question)               # Если нет - выдаем сгенерированный ответ нашей модели\n",
    "        emb_database = torch.cat((emb_database, emb), dim=0)            # Сохраняем эмбеддинг в БД\n",
    "        answers.append(answer)                                          # Сохраняем ответ от модели\n",
    "        update_db(answers, emb_database) # Обновляем БД\n",
    "        print(f'cos_sim={max_value}\\nОтвет из MODEL: {answer}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers, emb_database = load_db(answers_file_path, emb_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_sim=0.8721624612808228\n",
      "Ответ из DB: \"Стальной Щит\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Название компании\"\n",
    "pipe(question, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Завернем все в класс и попробуем запустить\n",
    "(Файл retrieval_augmented_generation.py)\n",
    "\n",
    "### TODO: Попробовать запустить в oobaboga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache_dir: E:/dev/0_MODELS_HF\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "from langchain.prompts import PromptTemplate\n",
    "from tqdm.auto import tqdm\n",
    "# import faiss\n",
    "\n",
    "cur_dir = Path.cwd()\n",
    "\n",
    "# utils\n",
    "cache_dir = 'E:/dev/0_MODELS_HF' if os.path.exists('E:/dev/0_MODELS_HF') else '~/.cache/huggingface/transformers'\n",
    "print('cache_dir:', cache_dir)\n",
    "\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = cache_dir\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# db\n",
    "answers_file_path = cur_dir.joinpath('database', 'answers.txt')\n",
    "emb_file_path = cur_dir.joinpath('database', 'emb_database.npy')\n",
    "\n",
    "# models\n",
    "base_model_name = \"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "adapt_model_name = \"IlyaGusev/saiga_mistral_7b_lora\"\n",
    "\n",
    "sentence_model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "\n",
    "class RAG_FOR_COMPANY:\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True, cache_dir=cache_dir)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model = AutoPeftModelForCausalLM.from_pretrained(adapt_model_name, device_map=device_map, torch_dtype=torch.bfloat16, cache_dir=cache_dir)\n",
    "        self.sent_tokenizer = AutoTokenizer.from_pretrained(sentence_model_name, cache_dir=cache_dir)\n",
    "        self.sent_model = AutoModel.from_pretrained(sentence_model_name, cache_dir=cache_dir)\n",
    "\n",
    "        self.answers, self.emb_database = self.load_db(answers_file_path, emb_file_path)\n",
    "\n",
    "        self.info_prompt_less10 = PromptTemplate.from_template('user: Компания \"Стальной Щит\" специализируется на разработке и производстве передовой военной техники. Наше предприятие расположено на улице Технологическая, 123, в городе Защитоград. Вы можете добраться до нас следующими способами: На метро: станция \"Вымышленная\" 10 выход, на автобусе: автобус №100 \"Атаковоево - Защитоград\". Мы занимаемся созданием инновационных боевых машин, включая танки, боевые вертолеты, беспилотные летательные аппараты и системы киберзащиты. Вы можете связаться с нами по телефону: +79876543210 или по почте: zashitograd@sb.ru. {question}\\nbot: Вот ответ на ваш вопрос длиной не более 10 слов:\"')\n",
    "\n",
    "    def get_embedding(self, sentence):\n",
    "        def _mean_pooling(model_output, attention_mask):\n",
    "            token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "        encoded_input = self.sent_tokenizer([sentence], padding=True, truncation=True, return_tensors='pt')     # Tokenize sentences\n",
    "        with torch.no_grad():                                                                                   # Compute token embeddings\n",
    "            model_output = self.sent_model(**encoded_input)\n",
    "        sentence_embeddings = _mean_pooling(model_output, encoded_input['attention_mask'])                      # Perform pooling\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)                                      # Normalize embeddings\n",
    "        return sentence_embeddings\n",
    "\n",
    "    def get_answer(self, question):\n",
    "        prompt = self.info_prompt_less10.format(question=question)   \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = self.model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), \n",
    "                                top_p=0.5,\n",
    "                                temperature=0.3,\n",
    "                                attention_mask=inputs[\"attention_mask\"],\n",
    "                                max_new_tokens=50,\n",
    "                                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                                do_sample=True)\n",
    "        output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        parsed_answer = output.split(\"Вот ответ на ваш вопрос длиной не более 10 слов:\")[1].strip()\n",
    "        if \"bot:\" in parsed_answer:\n",
    "            parsed_answer = parsed_answer.split(\"bot:\")[0].strip()\n",
    "        return parsed_answer\n",
    "\n",
    "    @staticmethod\n",
    "    def update_db(answers, emb_database):\n",
    "        with open(answers_file_path, 'w') as file:\n",
    "            file.writelines(answer + '\\n' for answer in answers)\n",
    "        np.save(emb_file_path, emb_database.numpy())\n",
    "\n",
    "    @staticmethod\n",
    "    def load_db(answers_file_path, emb_file_path):\n",
    "        if os.path.exists(answers_file_path) and os.path.exists(emb_file_path):\n",
    "            with open(answers_file_path, 'r') as file:\n",
    "                answers = [line.strip() for line in file.readlines()]\n",
    "            emb_database = torch.tensor(np.load(emb_file_path), dtype=torch.float32)\n",
    "        else:\n",
    "            answers = []\n",
    "            emb_database = torch.empty((0, 384), dtype=torch.float32)\n",
    "        return answers, emb_database\n",
    "    \n",
    "    def get_cos_sim(self, emb):\n",
    "        return F.cosine_similarity(self.emb_database, emb, dim=1, eps=1e-8)\n",
    "\n",
    "    def pipe(self, question):\n",
    "        emb = self.get_embedding(question)                                       # Создаем эмбеддинг вопроса\n",
    "        cos_sim = self.get_cos_sim(emb)\n",
    "        if cos_sim.numel() == 0:\n",
    "            cos_sim = torch.tensor([0.0])\n",
    "        max_value, max_index = torch.max(cos_sim, dim=0)                         # Получаем самый похожий вопрос и индекс сохраненного ответа\n",
    "\n",
    "        if max_value > 0.83:\n",
    "            source = 'DB'\n",
    "            answer = self.answers[max_index]                                     # Если есть похожий вопрос то выдаем закэшированный ответ из БД\n",
    "            return max_value, answer, source\n",
    "        else:\n",
    "            source = 'MODEL'\n",
    "            answer = self.get_answer(question)                                   # Если нет - выдаем сгенерированный ответ нашей модели\n",
    "            self.emb_database = torch.cat((self.emb_database, emb), dim=0)       # Сохраняем эмбеддинг в БД\n",
    "            self.answers.append(answer)                                          # Сохраняем ответ от модели\n",
    "            self.update_db(self.answers, self.emb_database)                      # Обновляем БД\n",
    "            return max_value, answer, source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d050fd967e6417cba5170edcde4fa76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cls = RAG_FOR_COMPANY()\n",
    "\n",
    "def get_response(question):\n",
    "    print('Question:', question)\n",
    "    max_value, answer, source = cls.pipe(question)\n",
    "    response = f'Answer: {answer}\\nCos_sim: {max_value}\\nSource: {source}\\n'\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заполним базу данных самыми частыми вопросами\n",
    "questions = [\n",
    "\"Какой адрес вашей компании?\",\n",
    "\"Где находится ваша компания?\",\n",
    "\"Какое местоположение вашей компании?\",\n",
    "\"Где точно находится ваша компания?\",\n",
    "\"Как добраться до вашей компании?\",\n",
    "\"Как мне добраться до вашей компании?\",\n",
    "\"На каком автобусе добраться до вашей компании?\",\n",
    "\"Какие автобусы едут до вашей компанииа?\",\n",
    "\"Что производят в вашей компании?\",\n",
    "\"Какая продукция производится в вашей компании?\",\n",
    "\"Какие товары производятся в вашей компании?\",\n",
    "\"Что именно производится в вашей компании?\",\n",
    "\"Какие изделия производятся в вашей компании?\",\n",
    "\"Какую продукцию я могу найти в вашей компании?\",\n",
    "\"Как можно с вами связаться?\",\n",
    "\"Напишите пожалуйста ваши контакты\",\n",
    "\"Дайте мне ваши контакты\",\n",
    "\"Дайте мне номер телефона\",\n",
    "\"Какая у вас почта?\",\n",
    "\"Какой номер телефона?\",\n",
    "\"Как с вами связаться?\",\n",
    "\"Напишите почту для связи\",\n",
    "\"Напиши почту и телефон\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f2891835bf4fe886ead2303c4fdaf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Какой адрес вашей компании?\n",
      "Answer: \"улица Технологическая, 123, город Защитоград\"\n",
      "Cos_sim: 0.0\n",
      "Source: MODEL\n",
      "\n",
      "\n",
      "Question: Где находится ваша компания?\n",
      "Answer: \"Улица Технологическая, 123, Защитоград\".\n",
      "Cos_sim: 0.7932148575782776\n",
      "Source: MODEL\n",
      "\n",
      "\n",
      "Question: Какое местоположение вашей компании?\n",
      "Answer: \"улица Технологическая, 123, город Защитоград\"\n",
      "Cos_sim: 0.8784106373786926\n",
      "Source: DB\n",
      "\n",
      "\n",
      "Question: Где точно находится ваша компания?\n",
      "Answer: \"Улица Технологическая, 123, Защитоград\".\n",
      "Cos_sim: 0.978456974029541\n",
      "Source: DB\n",
      "\n",
      "\n",
      "Question: Как добраться до вашей компании?\n",
      "Answer: \"Можно добраться на автобусе №100\".\n",
      "Cos_sim: 0.8171380162239075\n",
      "Source: MODEL\n",
      "\n",
      "\n",
      "Question: Как мне добраться до вашей компании?\n",
      "Answer: \"Можно добраться на автобусе №100\".\n",
      "Cos_sim: 0.9820703268051147\n",
      "Source: DB\n",
      "\n",
      "\n",
      "Question: На каком автобусе добраться до вашей компании?\n",
      "Answer: \"Можно добраться на автобусе №100\".\n",
      "Cos_sim: 0.9330138564109802\n",
      "Source: DB\n",
      "\n",
      "\n",
      "Question: Какие автобусы едут до вашей компанииа?\n",
      "Answer: \"Можно добраться на автобусе №100\".\n",
      "Cos_sim: 0.8617169857025146\n",
      "Source: DB\n",
      "\n",
      "\n",
      "Question: Что производят в вашей компании?\n",
      "Answer: \"Производим передовую военную технику\".\n",
      "Cos_sim: 0.8279297947883606\n",
      "Source: MODEL\n",
      "\n",
      "\n",
      "Question: Какая продукция производится в вашей компании?\n",
      "Answer: \"Производим передовую военную технику\".\n",
      "Cos_sim: 0.8586940765380859\n",
      "Source: DB\n",
      "\n",
      "\n",
      "Question: Какие товары производятся в вашей компании?\n",
      "Answer: \"Производим передовую военную технику\".\n",
      "Cos_sim: 0.9033495783805847\n",
      "Source: DB\n",
      "\n",
      "\n",
      "Question: Что именно производится в вашей компании?\n",
      "Answer: \"Производим передовую военную технику\".\n",
      "Cos_sim: 0.9505181908607483\n",
      "Source: DB\n",
      "\n",
      "\n",
      "Question: Какие изделия производятся в вашей компании?\n",
      "Answer: \"Производим передовую военную технику\".\n",
      "Cos_sim: 0.8364471197128296\n",
      "Source: DB\n",
      "\n",
      "\n",
      "Question: Какую продукцию я могу найти в вашей компании?\n",
      "Answer: \"Мы производим инновационные боевые машины\".\n",
      "Cos_sim: 0.8031903505325317\n",
      "Source: MODEL\n",
      "\n",
      "\n",
      "Question: Как можно с вами связаться?\n",
      "Answer: \"Мы можем связаться по телефону или по почте\".\n",
      "Cos_sim: 0.6237323880195618\n",
      "Source: MODEL\n",
      "\n",
      "\n",
      "Question: Напишите пожалуйста ваши контакты\n",
      "Answer: \"Стальной Щит\" расположен по адресу: улица Технологическая, 123, Защитоград.\n",
      "Cos_sim: 0.7078614234924316\n",
      "Source: MODEL\n",
      "\n",
      "\n",
      "Question: Дайте мне ваши контакты\n",
      "Answer: \"Стальной Щит\" расположен по адресу: улица Технологическая, 123, Защитоград.\n",
      "Cos_sim: 0.7967566251754761\n",
      "Source: MODEL\n",
      "\n",
      "\n",
      "Question: Дайте мне номер телефона\n",
      "Answer: \"+79876543210\"\n",
      "Cos_sim: 0.7152351140975952\n",
      "Source: MODEL\n",
      "\n",
      "\n",
      "Question: Какая у вас почта?\n",
      "Answer: \"zashitograd@sb.ru\"\n",
      "Cos_sim: 0.614043653011322\n",
      "Source: MODEL\n",
      "\n",
      "\n",
      "Question: Какой номер телефона?\n",
      "Answer: \"+79876543210\"\n",
      "Cos_sim: 0.9026669859886169\n",
      "Source: DB\n",
      "\n",
      "\n",
      "Question: Как с вами связаться?\n",
      "Answer: \"Мы можем связаться по телефону или по почте\".\n",
      "Cos_sim: 0.9292984008789062\n",
      "Source: DB\n",
      "\n",
      "\n",
      "Question: Напишите почту для связи\n",
      "Answer: \"Спасибо, мы свяжемся с вами в ближайшее время\"\n",
      "Cos_sim: 0.7192491888999939\n",
      "Source: MODEL\n",
      "\n",
      "\n",
      "Question: Напиши почту и телефон\n",
      "Answer: \"Стальной Щит\" расположен по адресу: улица Технологическая, 123, Защитоград.\n",
      "Cos_sim: 0.8023560047149658\n",
      "Source: MODEL\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for question in tqdm(questions):\n",
    "    get_response(question)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Как я могу с вами связаться?\n",
      "Answer: \"Мы можем связаться по телефону или по почте\".\n",
      "Cos_sim: 0.9212259650230408\n",
      "Source: DB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = 'Как я могу с вами связаться?'\n",
    "get_response(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Напишите почту для связи\n",
      "Answer: \"Спасибо, мы свяжемся с вами в ближайшее время\"\n",
      "Cos_sim: 0.9999999403953552\n",
      "Source: DB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = 'Напишите почту для связи'\n",
    "get_response(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":) Ответы можно/нужно править в файле answers.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
